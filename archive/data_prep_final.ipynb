{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import scipy as sci\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global vars\n",
    "samp_rt = 5120 #sampling rate Hz\n",
    "time_import = 7200 # time to analyse, seconds\n",
    "data_import = samp_rt * time_import # lines of data to import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_o_list(big_list, lil_size):\n",
    "    l_o_l = []\n",
    "    for i in range(0, len(big_list), lil_size):\n",
    "        lil = big_list[i : min(i + lil_size, len(big_list))]\n",
    "        l_o_l.append(lil)\n",
    "    return(l_o_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_analysis(sample , ttime, responsefrq =1, sampfrq = 5120):\n",
    "#     mmtic = time.perf_counter()\n",
    "    frqs = \"\"\n",
    "    amps = []\n",
    "    samp_int = 1/sampfrq\n",
    "    ft = np.fft.fft(sample)/len(sample)\n",
    "#     mmhic = time.perf_counter()\n",
    "    ft = ft[range(int(len(sample)/2))]\n",
    "    tpCount = len(sample)\n",
    "    values = np.arange(int(tpCount/2))\n",
    "    timePeriod = tpCount/sampfrq\n",
    "    frequencies = values/timePeriod\n",
    "    ft = abs(ft)\n",
    "#     mmcup = time.perf_counter()\n",
    "    #print(len(ft))\n",
    "    #print(len(frequencies))\n",
    "    feq = pd.DataFrame(data = {\"freq\":frequencies})\n",
    "#     l1 = time.perf_counter()\n",
    "    feq[\"amp\"] = ft\n",
    "#     l2 = time.perf_counter()\n",
    "    frqs = np.arange(0, 1 + int(feq.freq.max()), responsefrq)\n",
    "#     l3 = time.perf_counter()\n",
    "    #print(len(frqs))\n",
    "#     mmbless = time.perf_counter()\n",
    "    wow = feq.amp.rolling(responsefrq).mean()\n",
    "    yep = np.arange(responsefrq - 1, len(wow) + responsefrq  -1, responsefrq)\n",
    "    amps = wow[yep]\n",
    "#     l4 = time.perf_counter()\n",
    "#     mmtoc = time.perf_counter()\n",
    "#     print(\"init:\", mmhic-mmtic)\n",
    "#     print(\"fft:\", mmcup - mmhic, \"total:\", mmcup - mmtic)\n",
    "#     print(\"df1:\", mmbless - mmcup, \"total:\", mmbless - mmtic)\n",
    "#     print(\"done:\", mmtoc - mmbless, \"total:\", mmtoc - mmtic)\n",
    "#     print(\"l1, l2, l3, l4:\", l1-mmcup, l2-mmcup, l3-mmcup, l4-mmcup)\n",
    "#     print(\"for:\", l4 - l3)\n",
    "    return(frqs, amps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16 // 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(file):\n",
    "    df = pd.read_csv(file.path, header=15, names=[\"time\", \"acceleration\"])\n",
    "    readings = len(df)\n",
    "    transformable_readings  = readings // samp_rt\n",
    "    max_lines = (transformable_readings * samp_rt)\n",
    "    df2 = df.iloc[:(max_lines -1),:]\n",
    "#     df2 = df.iloc[:data_import,:]\n",
    "    l1 = list(df2.acceleration)\n",
    "    \n",
    "    print(\"l1\", len(l1))\n",
    "    l2 = list_o_list(l1, 5120)\n",
    "    del l1\n",
    "    print(\"l2\", len(l2))\n",
    "    bin_size = 10\n",
    "    tot_frqs = int((samp_rt / 2) / bin_size)\n",
    "    tic_a = time.perf_counter()\n",
    "    ft_l1 = np.empty((transformable_readings, tot_frqs))\n",
    "#     ft_l1 = np.empty((time_import, tot_frqs))\n",
    "    frq_l1 = np.empty(tot_frqs, np.int8)\n",
    "    try:\n",
    "        for c,v in enumerate(l2):\n",
    "            frq_l1, amp = freq_analysis(v, 1, bin_size, samp_rt) # 1 is the total time of the sample (1 second)\n",
    "            fail_c = c\n",
    "            ft_l1[c] = amp\n",
    "    except:\n",
    "        print(fail_c, fail_c/len(l2))\n",
    "    toc_a = time.perf_counter()\n",
    "    print(\"time diff:\", toc_a - tic_a)\n",
    "    direct = pd.DataFrame(data = ft_l1, columns = frq_l1)\n",
    "    l2_df = pd.DataFrame(data =l2)\n",
    "    del l2\n",
    "    direct[\"mean_acc\"] = l2_df.abs().mean(axis=1)\n",
    "    direct[\"median\"] = l2_df.abs().median(axis=1)\n",
    "    direct[\"95_acc\"] = l2_df.T.abs().quantile(.95)\n",
    "    direct[\"max_acc\"] = l2_df.abs().max(axis=1)\n",
    "    del l2_df\n",
    "    file_name = file.name[:-3] + \"big\"\n",
    "    direct.to_csv(file_name)\n",
    "    toe = time.perf_counter()\n",
    "    print(\"Finished in:\", toe - tic_a, \"seconds\")\n",
    "    return()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grey Ghost\\Documents\\GitHub\\project-sturm_drang\n",
      "C:\\Users\\Grey Ghost\\Documents\\GitHub\\project-sturm_drang\\data\n",
      "['C:\\\\Users\\\\Grey Ghost\\\\Documents\\\\GitHub\\\\project-sturm_drang', 'C:\\\\Users\\\\Grey Ghost\\\\Documents\\\\GitHub\\\\project-sturm_drang\\\\data', 'C:\\\\Users\\\\Grey Ghost\\\\Documents\\\\GitHub\\\\project-sturm_drang\\\\data\\\\.ipynb_checkpoints', 'C:\\\\Users\\\\Grey Ghost\\\\Documents\\\\GitHub\\\\project-sturm_drang\\\\data\\\\01-08-19-V118', 'C:\\\\Users\\\\Grey Ghost\\\\Documents\\\\GitHub\\\\project-sturm_drang\\\\data\\\\03-05-19-V118', 'C:\\\\Users\\\\Grey Ghost\\\\Documents\\\\GitHub\\\\project-sturm_drang\\\\data\\\\03-26-19-V118', 'C:\\\\Users\\\\Grey Ghost\\\\Documents\\\\GitHub\\\\project-sturm_drang\\\\data\\\\08-14-18-V118', 'C:\\\\Users\\\\Grey Ghost\\\\Documents\\\\GitHub\\\\project-sturm_drang\\\\data\\\\gps_files', 'C:\\\\Users\\\\Grey Ghost\\\\Documents\\\\GitHub\\\\project-sturm_drang\\\\data\\\\Test Train Sample Noise Data']\n"
     ]
    }
   ],
   "source": [
    "os.listdir()\n",
    "home = os.getcwd()\n",
    "print(home)\n",
    "da = home + \"\\data\"\n",
    "print(da)\n",
    "dirs = []\n",
    "dirs.append(home)\n",
    "dirs.append(da)\n",
    "with os.scandir(da) as contents:\n",
    "    for i in contents:\n",
    "        if i.is_dir():\n",
    "            dirs.append(i.path)\n",
    "print(dirs)\n",
    "                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DirEntry '.git'>\n",
      "<DirEntry '.gitignore'>\n",
      "<DirEntry '.idea'>\n",
      "<DirEntry '.ipynb_checkpoints'>\n",
      ".ipynb_checkpoints yes ch\n",
      "<DirEntry '01-08-19-V118_ch1.big'>\n",
      "01-08-19-V118_ch1.big yes ch\n",
      "<DirEntry '01-08-19-V118_ch2.big'>\n",
      "01-08-19-V118_ch2.big yes ch\n",
      "<DirEntry '03-05-19_ch1.big'>\n",
      "03-05-19_ch1.big yes ch\n",
      "<DirEntry '03-05-19_ch2.big'>\n",
      "03-05-19_ch2.big yes ch\n",
      "<DirEntry '03-26-19_ch1.big'>\n",
      "03-26-19_ch1.big yes ch\n",
      "<DirEntry '03-26-19_ch2.big'>\n",
      "03-26-19_ch2.big yes ch\n",
      "<DirEntry '08-14-18-V118_ch1.big'>\n",
      "08-14-18-V118_ch1.big yes ch\n",
      "<DirEntry '08-14-18-V118_ch2.big'>\n",
      "08-14-18-V118_ch2.big yes ch\n",
      "<DirEntry 'alt_modular.py'>\n",
      "<DirEntry 'bayesian_optimiser_tute.py'>\n",
      "<DirEntry 'capstone2_milestone 2.ipynb'>\n",
      "<DirEntry 'checkpoint.pth'>\n",
      "<DirEntry 'data'>\n",
      "<DirEntry 'data_prep.ipynb'>\n",
      "<DirEntry 'data_prep_development.ipynb'>\n",
      "<DirEntry 'data_prep_final.ipynb'>\n",
      "<DirEntry 'EDA.ipynb'>\n",
      "<DirEntry 'EDA2.ipynb'>\n",
      "<DirEntry 'eda2_b.ipynb'>\n",
      "<DirEntry 'EDA3.ipynb'>\n",
      "<DirEntry 'Figure 2020-11-08 143412.png'>\n",
      "<DirEntry 'Figure 2020-11-08 143421.png'>\n",
      "<DirEntry 'ft_first_3000.csv'>\n",
      "ft_first_3000.csv yes csv\n",
      "<DirEntry 'gps_inclusion.ipynb'>\n",
      "<DirEntry 'init_lstm_epoch_10.dat'>\n",
      "<DirEntry 'init_lstm_optim_state_epoch_10.dat'>\n",
      "<DirEntry 'init_lstm_state_epoch_10.dat'>\n",
      "<DirEntry 'linear_LSTM.py'>\n",
      "<DirEntry 'lstm.ipynb'>\n",
      "<DirEntry 'lstm2.py'>\n",
      "<DirEntry 'lstm2_data_prep.py'>\n",
      "<DirEntry 'lstm2_data_prep_double_sensor.py'>\n",
      "<DirEntry 'LSTM_classes.py'>\n",
      "<DirEntry 'LSTM_classes_functions.py'>\n",
      "<DirEntry 'lstm_examples.py'>\n",
      "<DirEntry 'lstm_model_01.py'>\n",
      "<DirEntry 'LSTM_modular.py'>\n",
      "<DirEntry 'LSTM_modular_combined.py'>\n",
      "<DirEntry 'LSTM_modular_two_sensor.py'>\n",
      "<DirEntry 'lstm_multivariate_working.py'>\n",
      "<DirEntry 'lstm_network.py'>\n",
      "<DirEntry 'LSTM_pipeline.py'>\n",
      "<DirEntry 'LSTM_sentiment_test.py'>\n",
      "<DirEntry 'lstm_training.ipynb'>\n",
      "<DirEntry 'main.py'>\n",
      "<DirEntry 'README.md'>\n",
      "<DirEntry 'requirements.txt'>\n",
      "<DirEntry 'snoop analysis.py'>\n",
      "<DirEntry 'speed_hist.png'>\n",
      "<DirEntry 'testing_Conv1D.ipynb'>\n",
      "<DirEntry 'torch_loader_tute.py'>\n",
      "<DirEntry 'track_num_hist.png'>\n",
      "<DirEntry 'track_speed_hist.png'>\n",
      "<DirEntry 'Untitled.ipynb'>\n",
      "<DirEntry 'Untitled1.ipynb'>\n",
      "<DirEntry 'Working_train_nn-Copy1.ipynb'>\n",
      "<DirEntry 'Working_train_nn.ipynb'>\n",
      "<DirEntry 'Working_train_nn_conv-Copy1.ipynb'>\n",
      "<DirEntry 'Working_train_nn_conv-Copy2.ipynb'>\n",
      "<DirEntry 'Working_train_nn_conv.ipynb'>\n",
      "<DirEntry '__pycache__'>\n",
      "<DirEntry '.ipynb_checkpoints'>\n",
      ".ipynb_checkpoints yes ch\n",
      "<DirEntry '01-08-19-V118'>\n",
      "<DirEntry '03-05-19-V118'>\n",
      "<DirEntry '03-26-19-V118'>\n",
      "<DirEntry '08-14-18-V118'>\n",
      "<DirEntry 'abs_np.npy'>\n",
      "<DirEntry 'Consolidated Snooper Log for Test train 6-188 - Mar. 5.xlsx'>\n",
      "<DirEntry 'fft_np.npy'>\n",
      "<DirEntry 'gps_files'>\n",
      "<DirEntry 'July 6, 2020 Vehicle 118 Expo Line Test Train Data.xlsx'>\n",
      "<DirEntry 'labels.txt'>\n",
      "<DirEntry 'M-Line and EVGL.zip'>\n",
      "<DirEntry 'NN_training.nn'>\n",
      "<DirEntry 'NN_training_big.nn'>\n",
      "<DirEntry 'NN_training_short.nn'>\n",
      "<DirEntry 'RE Location of accelerometer for vibration measurements.pdf'>\n",
      "<DirEntry 'reviews.txt'>\n",
      "<DirEntry 'Skytrain Linear Asset Database - Consolidated.xlsx'>\n",
      "<DirEntry 'SMC DATALOG FOR MAR. 5 TEST TRAIN 118-6.xlsx'>\n",
      "<DirEntry 'snoops.csv'>\n",
      "snoops.csv yes csv\n",
      "<DirEntry 'Switch Maint Works to date - April 2019.xlsx'>\n",
      "<DirEntry 'Test Train 118 Orientations.pdf'>\n",
      "<DirEntry 'Test Train Sample Noise Data'>\n",
      "<DirEntry 'Test Train Sample Noise Data.zip'>\n",
      "<DirEntry 'snoops-checkpoint.csv'>\n",
      "snoops-checkpoint.csv yes csv\n",
      "<DirEntry '.hubstorinfo'>\n",
      "<DirEntry '01-08-19-V118_ch1.csv'>\n",
      "01-08-19-V118_ch1.csv yes csv\n",
      "01-08-19-V118_ch1.csv yes ch\n",
      "re found\n",
      "l1 51353599\n",
      "l2 10030\n",
      "10028 0.9998005982053838\n",
      "time diff: 13.663688100001309\n",
      "Finished in: 23.348047399995266 seconds\n",
      "01-08-19-V118_ch1.csv yes\n",
      "<DirEntry '01-08-19-V118_ch2.csv'>\n",
      "01-08-19-V118_ch2.csv yes csv\n",
      "01-08-19-V118_ch2.csv yes ch\n",
      "re found\n",
      "l1 51353599\n",
      "l2 10030\n",
      "10028 0.9998005982053838\n",
      "time diff: 13.18056809999689\n",
      "Finished in: 22.910174999997253 seconds\n",
      "01-08-19-V118_ch2.csv yes\n",
      "<DirEntry 'ft_first_3000.csv'>\n",
      "ft_first_3000.csv yes csv\n",
      "<DirEntry '.hubstorinfo'>\n",
      "<DirEntry '.ipynb_checkpoints'>\n",
      ".ipynb_checkpoints yes ch\n",
      "<DirEntry '03-05-19_ch1.csv'>\n",
      "03-05-19_ch1.csv yes csv\n",
      "03-05-19_ch1.csv yes ch\n",
      "re found\n",
      "l1 59740159\n",
      "l2 11668\n",
      "11666 0.9998285910181693\n",
      "time diff: 15.445217899992713\n",
      "Finished in: 26.807964299994637 seconds\n",
      "03-05-19_ch1.csv yes\n",
      "<DirEntry '03-05-19_ch2.csv'>\n",
      "03-05-19_ch2.csv yes csv\n",
      "03-05-19_ch2.csv yes ch\n",
      "re found\n",
      "l1 59740159\n",
      "l2 11668\n",
      "11666 0.9998285910181693\n",
      "time diff: 15.766828699997859\n",
      "Finished in: 26.904292200008058 seconds\n",
      "03-05-19_ch2.csv yes\n",
      "<DirEntry 'ab_chan_swft.png'>\n",
      "ab_chan_swft.png yes ch\n",
      "<DirEntry 'all_abs_raw.png'>\n",
      "<DirEntry 'all_raw.png'>\n",
      "<DirEntry 'combined_raw.png'>\n",
      "<DirEntry 'first_leg.png'>\n",
      "<DirEntry 'first_leg_a.png'>\n",
      "<DirEntry 'first_leg_b.png'>\n",
      "<DirEntry 'ft_first_3000.csv'>\n",
      "ft_first_3000.csv yes csv\n",
      "<DirEntry 'invert_whole.png'>\n",
      "<DirEntry 'long_combo.png'>\n",
      "<DirEntry 'raw _short.png'>\n",
      "<DirEntry 'raw.png'>\n",
      "<DirEntry 'raw_downsampled.png'>\n",
      "<DirEntry 'speed_dep.png'>\n",
      "<DirEntry 'speed_hist.png'>\n",
      "<DirEntry 'track_num_hist.png'>\n",
      "<DirEntry 'track_speed_hist.png'>\n",
      "<DirEntry 'Untitled.ipynb'>\n",
      "<DirEntry '.hubstorinfo'>\n",
      "<DirEntry '03-26-19_ch1.csv'>\n",
      "03-26-19_ch1.csv yes csv\n",
      "03-26-19_ch1.csv yes ch\n",
      "re found\n",
      "l1 51788799\n",
      "l2 10115\n",
      "10113 0.9998022738507167\n",
      "time diff: 13.546472100002575\n",
      "Finished in: 23.1896257000044 seconds\n",
      "03-26-19_ch1.csv yes\n",
      "<DirEntry '03-26-19_ch2.csv'>\n",
      "03-26-19_ch2.csv yes csv\n",
      "03-26-19_ch2.csv yes ch\n",
      "re found\n",
      "l1 51788799\n",
      "l2 10115\n",
      "10113 0.9998022738507167\n",
      "time diff: 13.524503000007826\n",
      "Finished in: 23.362212699998054 seconds\n",
      "03-26-19_ch2.csv yes\n",
      "<DirEntry 'ft_first_3000.csv'>\n",
      "ft_first_3000.csv yes csv\n",
      "<DirEntry '.hubstorinfo'>\n",
      "<DirEntry '08-14-18-V118_ch1.csv'>\n",
      "08-14-18-V118_ch1.csv yes csv\n",
      "08-14-18-V118_ch1.csv yes ch\n",
      "re found\n",
      "l1 31728639\n",
      "l2 6197\n",
      "6195 0.999677263191867\n",
      "time diff: 8.295232000004034\n",
      "Finished in: 14.258042700006627 seconds\n",
      "08-14-18-V118_ch1.csv yes\n",
      "<DirEntry '08-14-18-V118_ch2.csv'>\n",
      "08-14-18-V118_ch2.csv yes csv\n",
      "08-14-18-V118_ch2.csv yes ch\n",
      "re found\n",
      "l1 31728639\n",
      "l2 6197\n",
      "6195 0.999677263191867\n",
      "time diff: 8.306294800000614\n",
      "Finished in: 14.285683599999174 seconds\n",
      "08-14-18-V118_ch2.csv yes\n",
      "<DirEntry 'ft_first_3000.csv'>\n",
      "ft_first_3000.csv yes csv\n",
      "<DirEntry '.hubstorinfo'>\n",
      "<DirEntry '.ipynb_checkpoints'>\n",
      ".ipynb_checkpoints yes ch\n",
      "<DirEntry '05-03-2019.gps'>\n",
      "<DirEntry 'August 14, 18.gps'>\n",
      "<DirEntry 'March 26, 19.gps'>\n",
      "<DirEntry 'Nov 27, 18.gps'>\n",
      "<DirEntry 'Oct 18, 18.gps'>\n",
      "<DirEntry 'Sept 06, 18.gps'>\n",
      "<DirEntry 'Sept 19, 18.gps'>\n",
      "<DirEntry '01-10-2019.txt'>\n",
      "<DirEntry '01-17-2019.txt'>\n",
      "<DirEntry '02-22-2019.txt'>\n",
      "<DirEntry '03-14-2019.txt'>\n",
      "<DirEntry '04-26-2018.txt'>\n",
      "<DirEntry '05-02-2019.txt'>\n",
      "<DirEntry '06-13-2019.txt'>\n",
      "<DirEntry '07-18-2019.txt'>\n",
      "<DirEntry '08-29-2019.txt'>\n",
      "<DirEntry '11-07-2019.txt'>\n"
     ]
    }
   ],
   "source": [
    "for i in dirs:\n",
    "    # print(i)\n",
    "    \n",
    "    with os.scandir(i) as stuff:\n",
    "        \n",
    "        for thing in stuff:\n",
    "            print(thing)\n",
    "            # print(thing.name, \"atribute\")\n",
    "            if re.search(\".csv\", thing.name):\n",
    "                print(thing.name, \"yes csv\")\n",
    "            if re.search(\"_ch\", thing.name):\n",
    "                print(thing.name, \"yes ch\")\n",
    "            if re.search(\".csv\", thing.name) and re.search(\"_ch\\d\", thing.name):\n",
    "                print(\"re found\")\n",
    "                try:\n",
    "                    process(thing)\n",
    "                    print(thing.name, \"yes\")\n",
    "                    \n",
    "                except:\n",
    "                    print(thing.name, \"- FAILED!\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
